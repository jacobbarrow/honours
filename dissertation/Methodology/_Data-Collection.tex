\subsection{Data Collection}
In order to create an algorithm to detect incongrouence in news articles, first news articles have to be collected and structured, and their suitability to the project determined. While there are already existing datasets, they tend to be incomplete, out of date, or in innacessiable formats, as they've been made to tackle different problems. For instance, while the Internet Archive has a catalogue of online news from around the world\footnote{\url{https://archive.org/details/ArchiveIt-Collection-11171}}, it is heavily fractured over thousands of files and is difficult to parse. Additionally, several datasets used to identify misleading and deceptive articles fabricate incongrouence by pairing the headline of one article with the body of another, such as in \citeA{park2020}. As mentioned in section \ref{existing-approaches}, this does not reflect the more subtle incongruity in real-world articles, so is not suited for this project.

To overcome these obstacles, a bespoke dataset needs to be created that can be classified and analysised by an NLP algorithm.

\subsubsection{Attributes}
Before collecting the data, it's important to decide what form it'll take and what attributes will be stored.

As the aim of the project is to identify incongruence between an articles headline and body, these two attributes will be included in the dataset. In order to identify trends and allow for further analysis, the article's date of publication and the publisher (e.g. BBC, The Guardian, etc.) will also be stored.

Collection could have gone further and retained the articles category (e.g. 'politics', 'sport' etc.), but different publishers categorise articles in different ways - for instance, the BBC has a combined 'Science and Environment' category, whereas The Guardian splits these into two distinct categories. Additionally, similar news articles can be filed under different categories, depending on the publisher. As this project's focus is on the article's content, and not categorisation, it can be considered out of scope to investigate the interplay between different publisher's approach to categorising articles.

\subsubsection{Sources}
The Independent is one of the only online publishers to make available their entire archives. Using the methods mentioned in Section \ref{obtaining-data}, 344,858 articles were collected, from 2011 to 2020. This 9-year period should prove a useful dataset to analyse a potentially changing landscape in the congruity of news headlines.

The BBC has an 'On This Day' page\footnote{\url{http://news.bbc.co.uk/onthisday}} that has a very select archive from 1950-2005, and analysing these articles could produce some interesting results. However, each of these articles will have been hand-picked (as evidenced by the 'In Context' notes alongside each article), and only represent historic world news events. Therefore, these articles will not be a suitable representation for the average of the time period they are from.

As well as archives, current news was also collected from a range of publishers. A varied range of UK publishers were selected, in order to create a cross

Table \ref{tab:data-sources} shows the full list of data sources collected, as well as the time range they cover and the total records obtained.

\begin{table}[h]
\begin{tabular}{lllll}
\textbf{Publisher} & \textbf{Earliest} & \textbf{Latest} & \textbf{Raw} \\
BBC On This Day & 1950-01-21 & 2005-12-11 & 1857 \\
The Independent & 2011-01-01 & 2018-05-28 & 317,135  \\
BBC & 2020-09-17 & 2021-01-01 & 4142  \\
Daily Mail & 2020-09-26 & 2021-01-02 & 14726 \\
The Guardian & 2020-09-17 & 2021-01-01 & 4078 \\
Huffington Post & 2020-10-09 & 2021-01-01 & 2920 \\
\end{tabular}
\caption{Extents of the data sources collected}
\label{tab:data-sources}
\end{table}

\subsubsection{Obtaining the Data} \label{obtaining-data}
To collect the data, several Python scripts were created. For the daily news, the publishers' various RSS feeds were consulted, and for the archives a more customised approach was taken.

These scripts utilise the BeautifulSoup library to parse each article's webpage and scrape them for the headline, date and body text. As each publisher builds their websites using different design patterns and with different technologies, each script had to be tailor made to fit the page structure. All the scripts used are available in this project's GitHub repository\footnote{\url{https://github.com/jacobbarrow/honours/tree/master/data-collection}}.

In addition, some sites implemented a strict rate-limit on requests - to make a copy of The Independent's archive took around XXX months to complete, scraping one article every 15 seconds. 

These scripts ran on a Raspberry Pi for around 100 days, from 2020-09-18 to 2020-01-02. Except for the Independent archive script, which ran continously, a cron job was used to run each script once per day at 1am.

\subsubsection{Ethics and Copyright}
Across a variety of datasets, 345 thousand articles were collected for analysis. This is a substantial amount of data, and represents the work of many individual journalists and news publishers. 

While automated techniques were used to collect the data, everything collected was publically accessiable. In addition, it is legal to make a digital copy of copyrighted data for non-commercial research \footnote{\url{https://www.gov.uk/guidance/exceptions-to-copyright#text-and-data-mining-for-non-commercial-research}}. Even so, care still needs to be taken in the obtainment of the data in order to avoid overloading or altering the regular service of these archives. As mentioned above, requests were rate-limited to avoid inadvertant denial of service attack, and spread out over a long period of time. Additionally, the rolling news was only collected once per day, at midnight, in order to minimise the impact of the scraping.

\subsubsection{Cleanliness}
While a bespoke scraper was created for each site, on some articles publishers used different page structures or included certain elements (such as inforgraphics) that the scraper didn't know how to handle. As a result, a portion of the articles in the dataset have erroneous text in them, such as unformatted lists of tweets or social media comments.

To obtain a measure of cleanliness, a subset of 300 articles was created. From this small sample, 36 (12\%) were corrupt or included content not part of the article's body text. Extrapolating this to the rest of the dataset, this means approximately 41,000 of the collected articles are 'dirty'. 

Cleaning the dataset is out of the scope of this project - the erroneous content doesn't follow a set pattern, and would be non-trivial to remove. Either human intervention or a well-trained neural network could be used to clean the dataset, or potentially a combination of both.

\subsubsection{Compilation}\label{data-compilation}
Once collected, the different sources were compiled into a single sqlite database using a Python script \footnote{\url{https://github.com/jacobbarrow/honours/blob/master/data-collection/compile.py}}. The database structure is described in table \ref{tab:data-compilation}.

\begin{table}[h]
\begin{tabular}{ll}
\textbf{Field} & \textbf{Description} \\
\texttt{id} & An auto-incrementing numerical id \\
\texttt{source} & The publisher the article was sourced from \\
\texttt{headline} & The article's headline \\
\texttt{body} & The article's body \\
\texttt{date} & The date the article was published \\
\end{tabular}
\caption{Database structure of compiled articles}
\label{tab:data-compilation}
\end{table}
