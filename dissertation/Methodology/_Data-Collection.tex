\subsection{Data Collection}
To create an algorithm to detect incongruence in news articles, news articles first have to be collected and structured, and their suitability to the project determined. While there are already existing datasets, they tend to be incomplete, out of date, or in inaccessible formats, as they have been made to tackle different problems. For instance, while the Internet Archive has a catalogue of online news from around the world\footnote{\url{https://archive.org/details/ArchiveIt-Collection-11171}}, it is heavily fractured over thousands of files and is difficult to parse. Additionally, several datasets used to identify misleading and deceptive articles fabricate incongruence by pairing one article's headline with the body of another, such as in \citeA{park2020}. As mentioned in section \ref{existing-approaches}, this does not reflect the more subtle incongruity in real-world articles, so is not suited for this project.

To overcome these obstacles, a bespoke dataset needs to be created that can be classified and analysised by an NLP algorithm.

\subsubsection{Attributes}
Before collecting the data, it is essential to decide what form it will take and what attributes will be stored.

As the project aims to identify incongruence between an article's headline and body, these two attributes will be included in the dataset. To identify trends and allow for further analysis, the article's date of publication and the publisher (e.g. BBC, The Guardian, etc.) will also be stored.

The collection could have gone further and retained the articles category (e.g. 'politics', 'sport'), but different publishers categorise articles in different ways - for instance, the BBC has a combined 'Science and Environment' category, whereas The Guardian splits these into two distinct categories. Additionally, similar news articles can be filed under different categories, depending on the publisher. As the focus is on the article's content and not categorisation, it can be considered out of scope to investigate the interplay between different publisher's approach to categorising articles.

\subsubsection{Sources}
The Independent is one of the only online publishers to make available their entire archives. Using the methods mentioned in Section \ref{obtaining-data}, 344,858 articles were collected, from 2011 to 2018. This 7-year period should prove a useful dataset to analyse a potentially changing landscape in news headlines' congruity.

The BBC has an 'On This Day' page\footnote{\url{http://news.bbc.co.uk/onthisday}} that has a very select archive from 1950-2005, and analysing these articles could produce some interesting results. However, each of these articles will have been hand-picked (as evidenced by the 'In Context' notes alongside each article) and only represent historic world news events. Therefore, these articles will not provide an unbiased representation of the period they encompass.

As well as archives, current news was also collected from a range of publishers. A varied range of UK publishers were selected to create a representative cross-section of the nation's media.

Table \ref{tab:data-sources} shows the full list of data sources collected, as well as the time range they cover and the total records obtained.

\begin{table}[h]
\begin{tabular}{lllll}
\textbf{Publisher} & \textbf{Earliest} & \textbf{Latest} & \textbf{Raw} \\
BBC On This Day & 1950-01-21 & 2005-12-11 & 1857 \\
The Independent & 2011-01-01 & 2018-05-28 & 317,135  \\
BBC & 2020-09-17 & 2021-01-01 & 4142  \\
Daily Mail & 2020-09-26 & 2021-01-02 & 14726 \\
The Guardian & 2020-09-17 & 2021-01-01 & 4078 \\
Huffington Post & 2020-10-09 & 2021-01-01 & 2920 \\
\end{tabular}
\caption{Extents of the data sources collected}
\label{tab:data-sources}
\end{table}

\subsubsection{Obtaining the Data} \label{obtaining-data}
Several Python scripts were created to collect the data. For the daily news, the publishers' various RSS feeds were consulted, and for the archives, a more customised approach was taken.

These scripts utilise the BeautifulSoup library to parse each article's webpage and scrape them for the headline, date and body text. Each script had to be tailor-made to fit the page structure as each publisher builds their websites using different design patterns and different technologies. All the scripts used are available in this project's GitHub repository\footnote{\url{https://github.com/jacobbarrow/honours/tree/master/data-collection}}.

Additionally, some sites implemented a strict rate-limit on requests - to make a copy of The Independent's archive took around six months to complete, scraping one article every 15 seconds. 

These scripts ran on a Raspberry Pi for around 100 days, from 2020-09-18 to 2020-01-02. Except for the Independent archive script, which ran continuously, a cron job was used to run each script once per day.

\subsubsection{Ethics and Copyright}
Across a variety of datasets, 345 thousand articles were collected for analysis, which is a substantial amount of data and represents many individual journalists and news publishers' work. 

While automated techniques were used to collect the data, everything collected was publically accessible. In addition, it is legal to make a digital copy of copyrighted data for non-commercial research \footnote{\url{https://www.gov.uk/guidance/exceptions-to-copyright#text-and-data-mining-for-non-commercial-research}}. Even so, care still needs to be taken in the obtainment of the data in order to avoid overloading or altering the regular service of these archives. As mentioned above, requests were rate-limited to avoid inadvertent denial of service attack and spread out over a long period of time. Additionally, the rolling news was only collected once per day, at 1 a.m., to minimise the scraping's impact.

\subsubsection{Cleanliness}
While a bespoke scraper was created for each site, on some articles, publishers used different page structures or included certain elements (such as infographics) that the scraper did not know how to handle. As a result, several collected articles have erroneous text in them, such as unformatted lists of tweets or social media comments.

To obtain a measure of cleanliness, a subset of 300 articles was created. From this small sample, 36 (12\%) were corrupt or included content not part of the article's body text. Extrapolating this to the rest of the dataset means approximately 41,000 of the collected articles are 'dirty'. 

Cleaning the dataset is out of this project's scope - the erroneous content does not follow a set pattern and would be non-trivial to remove. Either human intervention or a well-trained neural network could be used to clean the dataset, or potentially a combination of both.

\subsubsection{Compilation}\label{data-compilation}
Once collected, the different sources were compiled into a single SQLite database using a Python script \footnote{\url{https://github.com/jacobbarrow/honours/blob/master/data-collection/compile.py}}. The database structure is described in table \ref{tab:data-compilation}.

\begin{table}[h]
\begin{tabular}{ll}
\textbf{Field} & \textbf{Description} \\
\texttt{id} & An auto-incrementing numerical id \\
\texttt{source} & The publisher the article was sourced from \\
\texttt{headline} & The article's headline \\
\texttt{body} & The article's body \\
\texttt{date} & The date the article was published \\
\end{tabular}
\caption{Database structure of compiled articles}
\label{tab:data-compilation}
\end{table}

